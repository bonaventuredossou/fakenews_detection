{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Fake_News.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCy4XBOsK5ho"
      },
      "source": [
        "Adapted Implementation of Learning Structured Representation for Text Classification via Reinforcement Learning (https://ojs.aaai.org/index.php/AAAI/article/view/12047)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoIF0DDyk-xW"
      },
      "source": [
        "# Bonaventure Dossou"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArtcIyuJEBNC",
        "outputId": "48d70483-ffbd-4c37-9310-30c879cc16d2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import pandas as pd\n",
        "from torchtext.legacy import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors\n",
        "import torch.nn as nn\n",
        "from gensim.models import Word2Vec\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "models = '/content/drive/MyDrive/models/'\n",
        "plots = '/content/drive/MyDrive/plots/'\n",
        "\n",
        "path = '/content/drive/MyDrive/data_label_1.csv'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snyIFw-m0t8A"
      },
      "source": [
        "def load_dataset(filename, batch_size):\n",
        "    \n",
        "    # set_ = pd.read_csv(filename)\n",
        "    # set_.dropna(axis=0, how='any', inplace=True)\n",
        "    # set_.drop_duplicates(keep=False, inplace=True)\n",
        "    # set_['Length'] = set_['text'].apply(lambda x: len(x.strip().split()))\n",
        "\n",
        "    # set_ = set_[set_['Length'] <= 150]\n",
        "\n",
        "    # c_filename = '/content/drive/MyDrive/data_label_1.csv'\n",
        "    # set_.to_csv(c_filename, index=False)\n",
        "\n",
        "    tokenize = lambda x: x.split()\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True)\n",
        "    LABEL = data.LabelField()\n",
        "\n",
        "    fields = [('text', TEXT), ('label', LABEL)]\n",
        "\n",
        "    dataset = data.TabularDataset(path=filename, format='csv', fields=fields, skip_header = True)\n",
        "    train_data, test_data = dataset.split(split_ratio=0.9, stratified=True, strata_field='label')\n",
        "    # 90% - 10% (Train, Test)\n",
        "    TEXT.build_vocab(train_data, vectors=\"glove.6B.300d\")\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "    print (\"Vocabulary size: {}\".format(len(TEXT.vocab)))\n",
        "    print (\"Vector Shape: {}\".format(TEXT.vocab.vectors.size()))\n",
        "    print (\"Number of labels: {}\".format(len(LABEL.vocab)))\n",
        "\n",
        "    train_data, valid_data = train_data.split(split_ratio=0.9, stratified=True, strata_field='label') # Further splitting of training_data to create new training_data & validation_data\n",
        "    # 90% - 10% (Real Training, Validation)\n",
        "\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
        "\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ4Xm4jMKvfH"
      },
      "source": [
        "Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_J91sREKs8e"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "batch_size = 5\n",
        "global_batch_size = 5\n",
        "output_size = 2\n",
        "hidden_size = 300\n",
        "embedding_length = 300\n",
        "samplecnt = 5\n",
        "epsilon = 0.05\n",
        "maxlength = 150\n",
        "alpha = 0.1\n",
        "tau = 0.1\n",
        "delay_critic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtNsyvWSLoVj"
      },
      "source": [
        "Simple LSTM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omPurKHlKIcu",
        "outputId": "434f3257-9df1-4915-e04a-92bb558931bb"
      },
      "source": [
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset(path, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 35984\n",
            "Vector Shape: torch.Size([35984, 300])\n",
            "Number of labels: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1XiUJnXfCSM"
      },
      "source": [
        "Early Stopping to Control Under/Over fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd9yNaVdfATl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience, path, verbose=True, delta=0, trace_func=print):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
        "        # torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQELrmenLsOJ"
      },
      "source": [
        "class FakeNewsClassifier(nn.Module):\n",
        "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "        super(FakeNewsClassifier, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_length = embedding_length\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
        "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "        self.Dropout = nn.Dropout(0.3)\n",
        "        self.label = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sentence, batch_size=None):\n",
        "        input = self.word_embeddings(input_sentence)\n",
        "        input = input.permute(1, 0, 2)\n",
        "        batch_size = input.size(1)\n",
        "        if batch_size is None:\n",
        "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())  # Initial hidden state of the LSTM\n",
        "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())  # Initial cell state of the LSTM\n",
        "        else:\n",
        "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "        final_output = self.label(self.Dropout(final_hidden_state[\n",
        "                                                   -1]))  # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def wordvector_find(self, x):\n",
        "        return self.word_embeddings(x)\n",
        "\n",
        "    def getNextHiddenState(self, hc, x):\n",
        "        hidden = hc[0, 0:self.hidden_size].view(1, 1, self.hidden_size)\n",
        "        cell = hc[0, self.hidden_size:].view(1, 1, self.hidden_size)\n",
        "        input = self.word_embeddings(x).view(1, 1, -1)\n",
        "        out, hidden = self.lstm(input, (hidden, cell))\n",
        "        hidden = torch.cat((hidden[0], hidden[1]), -1).view(1, -1)\n",
        "        return self.Dropout(out), self.Dropout(hidden)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-6WxGiukdK3",
        "outputId": "80bdf869-a523-4942-db5c-c708be4abbe4"
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "def Sampling_RL(actor, critic, inputs, vector, length, epsilon, Random = True):\n",
        "    current_lower_state = torch.zeros(1,2*hidden_size).cuda()\n",
        "    actions = []\n",
        "    states = []\n",
        "    for pos in range(length):\n",
        "        predicted = actor.get_target_output(current_lower_state, vector[0][pos], scope = \"target\")\n",
        "        states.append([current_lower_state, vector[0][pos]])\n",
        "        if Random:\n",
        "            if random.random() > epsilon:\n",
        "                action = (0 if random.random() < float(predicted[0].data.detach().cpu().numpy().item()) else 1)\n",
        "            else:\n",
        "                action = (1 if random.random() < float(predicted[0].data.detach().cpu().numpy().item()) else 0)\n",
        "        else:\n",
        "            action = np.argmax(predicted.data.detach().cpu().numpy()).item()\n",
        "        actions.append(action)\n",
        "        if action == 1:\n",
        "            out_d, current_lower_state = critic.forward_lstm(current_lower_state, inputs[0][pos], scope = \"target\")\n",
        "    Rinput = []\n",
        "    for (i, a) in enumerate(actions):\n",
        "        if a == 1:\n",
        "            Rinput.append(int(inputs[0][i].item())) ####\n",
        "    Rlength = len(Rinput)\n",
        "    if Rlength == 0:\n",
        "        actions[length-2] = 1\n",
        "        Rinput.append(inputs[0][length-2])\n",
        "        Rlength = 1\n",
        "    Rinput += [1] * (maxlength - Rlength)\n",
        "    Rinput = torch.tensor(Rinput).view(1,-1).cuda()    \n",
        "    return actions, states, Rinput, Rlength\n",
        "\n",
        "\n",
        "class policyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(policyNet, self).__init__()\n",
        "        self.hidden = hidden_size\n",
        "        self.W1 = nn.Parameter(torch.cuda.FloatTensor(2*self.hidden, 1).uniform_(-0.5, 0.5)) \n",
        "        self.W2 = nn.Parameter(torch.cuda.FloatTensor(embedding_length, 1).uniform_(-0.5, 0.5)) \n",
        "        self.b = nn.Parameter(torch.cuda.FloatTensor(1, 1).uniform_(-0.5, 0.5))\n",
        "\n",
        "    def forward(self, h, x):\n",
        "        h_ = torch.matmul(h.view(1,-1), self.W1) # 1x1\n",
        "        x_ = torch.matmul(x.view(1,-1), self.W2) # 1x1\n",
        "        scaled_out = torch.sigmoid(h_ +  x_ + self.b) # 1x1\n",
        "        scaled_out = torch.clamp(scaled_out, min=1e-5, max=1 - 1e-5)\n",
        "        scaled_out = torch.cat([1.0 - scaled_out, scaled_out],0)\n",
        "        return scaled_out\n",
        "\n",
        "\n",
        "\n",
        "class critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(critic, self).__init__()\n",
        "        self.target_pred = FakeNewsClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "        self.active_pred = FakeNewsClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "\n",
        "\n",
        "    def forward(self, x, scope):\n",
        "        if scope == \"target\":\n",
        "            out = self.target_pred(x)\n",
        "        if scope == \"active\":\n",
        "            out = self.active_pred(x)\n",
        "        return out\n",
        "\n",
        "    def assign_target_network(self):\n",
        "        params = []\n",
        "        for name, x in self.active_pred.named_parameters():\n",
        "            params.append(x)\n",
        "        i=0\n",
        "        for name, x in self.target_pred.named_parameters():\n",
        "            x.data = deepcopy(params[i].data)\n",
        "            i+=1\n",
        "\n",
        "    def update_target_network(self):\n",
        "        params = []\n",
        "        for name, x in self.active_pred.named_parameters():\n",
        "            params.append(x)\n",
        "        i=0\n",
        "        for name, x in self.target_pred.named_parameters():\n",
        "            x.data = deepcopy(params[i].data * (tau) + x.data * (1-tau))\n",
        "            i+=1\n",
        "\n",
        "    def assign_active_network(self):\n",
        "        params = []\n",
        "        for name, x in self.target_pred.named_parameters():\n",
        "            params.append(x)\n",
        "        i=0\n",
        "        for name, x in self.active_pred.named_parameters():\n",
        "            x.data = deepcopy(params[i].data)\n",
        "            i+=1\n",
        "\n",
        "    def assign_active_network_gradients(self):\n",
        "        params = []\n",
        "        for name, x in self.target_pred.named_parameters():\n",
        "            params.append(x)\n",
        "        i=0\n",
        "        for name, x in self.active_pred.named_parameters():\n",
        "            x.grad = deepcopy(params[i].grad)\n",
        "            i+=1\n",
        "        for name, x in self.target_pred.named_parameters():\n",
        "            x.grad = None\n",
        "\n",
        "    def forward_lstm(self, hc, x, scope):\n",
        "        if scope == \"target\":\n",
        "            out, state = self.target_pred.getNextHiddenState(hc, x)\n",
        "        if scope == \"active\":\n",
        "            out, state = self.active_pred.getNextHiddenState(hc, x)\n",
        "        return out, state\n",
        "\n",
        "    def wordvector_find(self, x):\n",
        "        return self.target_pred.wordvector_find(x)\n",
        "\n",
        "\n",
        "class actor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(actor, self).__init__()\n",
        "        self.target_policy = policyNet()\n",
        "        self.active_policy = policyNet()\n",
        "        \n",
        "    def get_target_logOutput(self, h, x):\n",
        "        out = self.target_policy(h, x)\n",
        "        logOut = torch.log(out)\n",
        "        return logOut\n",
        "\n",
        "    def get_target_output(self, h, x, scope):\n",
        "        if scope == \"target\":\n",
        "            out = self.target_policy(h, x)\n",
        "        if scope == \"active\":\n",
        "            out = self.active_policy(h, x)\n",
        "        return out\n",
        "\n",
        "    def get_gradient(self, h, x, reward, scope):\n",
        "        if scope == \"target\":\n",
        "            out = self.target_policy(h, x)\n",
        "            logout = torch.log(out).view(-1)\n",
        "            index = reward.index(0)\n",
        "            index = (index + 1) % 2\n",
        "            grad = torch.autograd.grad(logout[index].view(-1), self.target_policy.parameters())\n",
        "            grad[0].data = grad[0].data * reward[index]\n",
        "            grad[1].data = grad[1].data * reward[index]\n",
        "            grad[2].data = grad[2].data * reward[index]\n",
        "            return grad\n",
        "        if scope == \"active\":\n",
        "            out = self.active_policy(h, x)\n",
        "        return out\n",
        "\n",
        "    def assign_active_network_gradients(self, grad1, grad2, grad3):\n",
        "        params = [grad1, grad2, grad3]    \n",
        "        i=0\n",
        "        for name, x in self.active_policy.named_parameters():\n",
        "            x.grad = deepcopy(params[i])\n",
        "            i+=1\n",
        "\n",
        "    def update_target_network(self):\n",
        "        params = []\n",
        "        for name, x in self.active_policy.named_parameters():\n",
        "            params.append(x)\n",
        "        i=0\n",
        "        for name, x in self.target_policy.named_parameters():\n",
        "            x.data = deepcopy(params[i].data * (tau) + x.data * (1-tau))\n",
        "            i+=1\n",
        "\n",
        "    def assign_active_network(self):\n",
        "        params = []\n",
        "        for name, x in self.target_policy.named_parameters():\n",
        "            params.append(x)\n",
        "        i=0\n",
        "        for name, x in self.active_policy.named_parameters():\n",
        "            x.data = deepcopy(params[i].data)\n",
        "            i+=1\n",
        "\n",
        "def train_model(criticModel, actorModel, train_iter, epoch, RL_train = True, LSTM_train = True):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    criticModel.cuda()\n",
        "    actorModel.cuda()\n",
        "    critic_target_optimizer = torch.optim.SGD(criticModel.target_pred.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    critic_active_optimizer = torch.optim.SGD(criticModel.active_pred.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "    actor_target_optimizer = torch.optim.SGD(actorModel.target_policy.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    actor_active_optimizer = torch.optim.SGD(actorModel.active_policy.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    steps = 0\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        totloss = 0.\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        lengths = batch.text[1]\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        pred = torch.zeros(batch_size, 2).cuda()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] is not batch_size):\n",
        "            continue\n",
        "\n",
        "        criticModel.assign_active_network()\n",
        "        actorModel.assign_active_network()\n",
        "\n",
        "        avgloss = 0\n",
        "        aveloss = 0.\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            x = text[i].view(1,-1)\n",
        "            y = target[i].view(1)\n",
        "            length = int(lengths[i])\n",
        "\n",
        "            if RL_train:\n",
        "                criticModel.train(False)\n",
        "                actorModel.train()\n",
        "                actionlist, statelist, losslist = [], [], []\n",
        "                aveLoss = 0.\n",
        "\n",
        "                for i in range(samplecnt):\n",
        "                    actions, states, Rinput, Rlength = Sampling_RL(actorModel, criticModel, x, criticModel.wordvector_find(x), length, epsilon, Random=True)\n",
        "                    actionlist.append(actions)\n",
        "                    statelist.append(states)\n",
        "                    out = criticModel(Rinput, scope = \"target\")\n",
        "                    loss_ = loss_fn(out, y)\n",
        "                    loss_ += (float(Rlength) / length) **2 *0.15\n",
        "                    aveloss += loss_\n",
        "                    losslist.append(loss_)\n",
        "\n",
        "                aveloss /= samplecnt\n",
        "                totloss += aveloss\n",
        "                grad1 = None\n",
        "                grad2 = None\n",
        "                grad3 = None\n",
        "                flag = 0 \n",
        "\n",
        "                if LSTM_train:\n",
        "                    criticModel.train()\n",
        "                    actorModel.train()  \n",
        "                    critic_active_optimizer.zero_grad()\n",
        "                    critic_target_optimizer.zero_grad()\n",
        "                    prediction = criticModel(Rinput, scope = \"target\")\n",
        "                    pred[i] = prediction\n",
        "                    loss = loss_fn(prediction, y)\n",
        "                    loss.backward()\n",
        "\n",
        "                    criticModel.assign_active_network_gradients()\n",
        "                    critic_active_optimizer.step()\n",
        "\n",
        "                for i in range(samplecnt):\n",
        "                    for pos in range(len(actionlist[i])):\n",
        "                        rr = [0, 0]\n",
        "                        rr[actionlist[i][pos]] = ((losslist[i] - aveloss) * alpha).cpu().item()\n",
        "                        g = actorModel.get_gradient(statelist[i][pos][0], statelist[i][pos][1], rr, scope = \"target\")\n",
        "                        if flag == 0:\n",
        "                            grad1 = g[0]\n",
        "                            grad2 = g[1]\n",
        "                            grad3 = g[2]\n",
        "                            flag = 1\n",
        "                        else:\n",
        "                            grad1 += g[0]\n",
        "                            grad2 += g[1]\n",
        "                            grad3 += g[2]\n",
        "                actor_target_optimizer.zero_grad()\n",
        "                actor_active_optimizer.zero_grad()\n",
        "                actorModel.assign_active_network_gradients(grad1, grad2, grad3)\n",
        "                actor_active_optimizer.step()\n",
        "            else: \n",
        "                criticModel.train()\n",
        "                actorModel.train(False)  \n",
        "                critic_active_optimizer.zero_grad()\n",
        "                critic_target_optimizer.zero_grad()\n",
        "                prediction = criticModel(x, scope = \"target\")\n",
        "                pred[i] = prediction\n",
        "                loss = loss_fn(prediction, y)\n",
        "                avgloss += loss.item()\n",
        "                loss.backward()\n",
        "                criticModel.assign_active_network_gradients()\n",
        "                critic_active_optimizer.step()\n",
        "        \n",
        "        if RL_train:\n",
        "            criticModel.train(False)\n",
        "            actorModel.train()\n",
        "            actorModel.update_target_network()\n",
        "            if LSTM_train:\n",
        "                criticModel.train()\n",
        "                actorModel.train() \n",
        "                criticModel.update_target_network()                \n",
        "        else:\n",
        "            criticModel.train()\n",
        "            actorModel.train(False)  \n",
        "            criticModel.assign_target_network()\n",
        "        avgloss /= batch_size\n",
        "        num_corrects = (torch.max(pred, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        steps += 1\n",
        "                \n",
        "        total_epoch_loss += avgloss\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    y_pred, y_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            if (text.size()[0] is not batch_size):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text, scope = \"target\")\n",
        "\n",
        "            pred_idx = torch.max(prediction, 1)[1]\n",
        "            y_true += list(target.data.detach().cpu().numpy())\n",
        "            y_pred += list(pred_idx.detach().cpu().numpy())\n",
        "\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter), classification_report(y_true, y_pred)\n",
        "\n",
        "def eval_model_RL(criticModel, actorModel, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    criticModel.eval()\n",
        "    actorModel.eval()\n",
        "    y_pred, y_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            if (text.size()[0] is not batch_size):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            lengths = batch.text[1]\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            batch_loss = 0\n",
        "            pred = torch.zeros(batch_size, 2).cuda()\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                x = text[i].view(1,-1)\n",
        "                y = target[i].view(1)\n",
        "                length = int(lengths[i])\n",
        "\n",
        "                actions, states, Rinput, Rlenth = Sampling_RL(actorModel, criticModel, x, criticModel.wordvector_find(x), length, epsilon, Random=False)\n",
        "                \n",
        "                prediction = criticModel(Rinput, scope = \"target\")\n",
        "                loss = loss_fn(prediction, y)\n",
        "                batch_loss += loss\n",
        "                pred[i] = prediction\n",
        "\n",
        "            pred_idx = torch.max(pred, 1)[1]\n",
        "            y_true += list(target.data.detach().cpu().numpy())\n",
        "            y_pred += list(pred_idx.detach().cpu().numpy())\n",
        "\n",
        "            num_corrects = (torch.max(pred, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += batch_loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter), classification_report(y_true, y_pred)\n",
        "\n",
        "criticModel = critic()\n",
        "actorModel = actor()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using {} to train\".format(device))\n",
        "criticModel.to(device)\n",
        "actorModel.to(device)\n",
        "\n",
        "actorModel.cuda()\n",
        "criticModel.cuda()\n",
        "\n",
        "loss_fn = F.cross_entropy\n",
        "best_val_acc = 0.\n",
        "\n",
        "epoch = 30"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6k8O7a0tUJR"
      },
      "source": [
        "# Training the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxomwhjZtUsH"
      },
      "source": [
        "lstm_train_loss, lstm_train_acc, lstm_val_loss, lstm_val_acc = [], [], [], [] \n",
        "if delay_critic:\n",
        "    print(\"Training Classifier\")\n",
        "    early_stopping = EarlyStopping(patience=10, path='')\n",
        "    for epoch in range(epoch):\n",
        "        train_loss, train_acc = train_model(criticModel, actorModel, train_iter, epoch, RL_train = False)\n",
        "        val_loss, val_acc, _= eval_model(criticModel, valid_iter)\n",
        "        lstm_train_loss.append(train_loss)\n",
        "        lstm_train_acc.append(train_acc)\n",
        "        lstm_val_loss.append(val_loss)\n",
        "        lstm_val_acc.append(val_acc)\n",
        "        if val_acc > best_val_acc:\n",
        "            torch.save(criticModel.state_dict(), models+'classifier.pt')\n",
        "            best_val_acc = val_acc\n",
        "            print(\"Saved Classifier Model with acc: \", val_acc)\n",
        "        \n",
        "        early_stopping(val_loss, criticModel)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GohZqXGgzIol"
      },
      "source": [
        "Classifier Performance on Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUWtPr2szIBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe7af61-f548-4c16-8743-8bac25fe8d0e"
      },
      "source": [
        "test_loss, test_acc, class_report = eval_model(criticModel, test_iter)\n",
        "print(\"Classifier Performance on Testing Set: {}\".format(test_acc))\n",
        "print(\"\\n\\n\")\n",
        "print(\"Classifier Classification Report on Testing Set\\n: {}\".format(class_report))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier Performance on Testing Set: 99.68911917098445\n",
            "\n",
            "\n",
            "\n",
            "Classifier Classification Report on Testing Set\n",
            ":               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       724\n",
            "           1       1.00      0.99      0.99       241\n",
            "\n",
            "    accuracy                           1.00       965\n",
            "   macro avg       1.00      1.00      1.00       965\n",
            "weighted avg       1.00      1.00      1.00       965\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDtPSiPBt2am"
      },
      "source": [
        "Plot Classifier Training Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1r-pFndvSAB"
      },
      "source": [
        "Use trained Classifier for the RL agent training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0fllZnWvRi6"
      },
      "source": [
        "best_val_acc1 = 0\n",
        "rl_train_loss, rl_train_acc, rl_val_loss, rl_val_acc = [], [], [], []\n",
        "criticModel.load_state_dict(torch.load(models+'classifier.pt'))\n",
        "early_stopping = EarlyStopping(patience=10, path='')\n",
        "for i in range(30):\n",
        "    print(\"Epoch: {}\".format(i+1))\n",
        "    train_loss, train_acc = train_model(criticModel, actorModel, train_iter, i, LSTM_train = False)\n",
        "    val_loss, val_acc, _= eval_model_RL(criticModel, actorModel,  valid_iter)\n",
        "    rl_train_loss.append(train_loss)\n",
        "    rl_train_acc.append(train_acc)\n",
        "    rl_val_loss.append(val_loss)\n",
        "    rl_val_acc.append(val_acc)\n",
        "    if val_acc > best_val_acc1:\n",
        "        torch.save(actorModel.state_dict(), models+'rl_agent.pt')\n",
        "        best_val_acc1 = val_acc\n",
        "\n",
        "    early_stopping(val_loss, criticModel)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    print(f'Epoch: {i+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzMCkVF15EUe"
      },
      "source": [
        "RL Agent Performance on the Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvH76OD34u03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6565e851-9f49-4d2d-8c60-cae31cf3eda0"
      },
      "source": [
        "criticModel.load_state_dict(torch.load(models+'classifier.pt'))\n",
        "actorModel.load_state_dict(torch.load(models+'rl_agent.pt'))\n",
        "\n",
        "test_loss, test_acc, class_report = eval_model_RL(criticModel, actorModel, test_iter)\n",
        "print(\"RL Agent Performance on Testing Set: {}\".format(test_acc))\n",
        "print(\"\\n\\n\")\n",
        "print(\"RL Agent Classification Report on Testing Set\\n: {}\".format(class_report))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RL Agent Performance on Testing Set: 99.89637305699482\n",
            "\n",
            "\n",
            "\n",
            "RL Agent Classification Report on Testing Set\n",
            ":               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       724\n",
            "           1       1.00      1.00      1.00       241\n",
            "\n",
            "    accuracy                           1.00       965\n",
            "   macro avg       1.00      1.00      1.00       965\n",
            "weighted avg       1.00      1.00      1.00       965\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}